{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The windows are meant to be run in order from top to bottom\n",
    "#This window is for creating a list of the most common words used by each character throughout the series\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from heapq import nlargest\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "master_array = [] #append results to array to create data frame\n",
    "mostcommon = {}\n",
    "mostcommon[\"Monica\"] = {}\n",
    "mostcommon[\"Rachel\"] = {}\n",
    "mostcommon[\"Phoebe\"] = {}\n",
    "mostcommon[\"Chandler\"] = {}\n",
    "mostcommon[\"Ross\"] = {}\n",
    "mostcommon[\"Joey\"] = {}\n",
    "for filename in os.listdir('season/'):\n",
    "    if filename.endswith(\".html\"):\n",
    "        split_name = filename.split('.') #obtain the season and episodes\n",
    "        season = split_name[0]   \n",
    "        episode = split_name[1]\n",
    "        #TODO method to get each line of the script\n",
    "        master_array.append([season, episode])\n",
    "    else:\n",
    "        pass\n",
    "for num, ep in enumerate(master_array):\n",
    "    filename=master_array[num][0]+'.'+master_array[num][1]\n",
    "\n",
    "    f = open('season/'+filename, 'r')\n",
    "    data = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(data)\n",
    "    readsoup = BeautifulSoup(data)\n",
    "\n",
    "    soup = soup.get_text().translate(str.maketrans({'\\n': ' ', '\\xa0': ''}))\n",
    "    pattern = re.compile(r'\\s(?=\\w+(?=:))') # store the regex\n",
    "    result = re.split(pattern, soup) # split the script where our pattern matched (pink dot)\n",
    "\n",
    "\n",
    "    episode_array=[]\n",
    "    for item in result:\n",
    "        split_line = item.split(': ')\n",
    "        try:\n",
    "            character = split_line[0]\n",
    "            speech = split_line[1]\n",
    "            episode_array.append([character, speech])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    df = pd.DataFrame(episode_array, columns = ['Person','Said'])\n",
    "\n",
    "    df['Person'].replace({'CHAN':'Chandler','CHANDLER':'Chandler', 'Chandlers':'Chandler',\n",
    "                        'JOEY':'Joey',\n",
    "                        'MNCA':'Monica','MONICA':'Monica',\n",
    "                        'PHOE':'Phoebe','PHOEBE':'Phoebe', 'Pheebs':'Phoebe',\n",
    "                        'Rache':'Rachel','RACHEL':'Rachel', 'RACH':'Rachel',\n",
    "                        'ROSS':'Ross'},inplace=True)\n",
    "\n",
    "    curved = re.compile(\"(\\(.*?\\))\")\n",
    "    brackets = re.compile(\"(\\[.*?\\])\")\n",
    "    scene = re.compile(\"\\[Scene\")\n",
    "    for j,i in enumerate(df['Said']):\n",
    "        df.iloc[j][1] =re.sub(curved, '', i)\n",
    "\n",
    "    for j,i in enumerate(df['Said']):\n",
    "        df.iloc[j][1] =re.sub(brackets, '', i)\n",
    "        \n",
    "    for j,i in enumerate(df['Said']):\n",
    "        df.iloc[j][1] =re.sub(scene, '', i)\n",
    "\n",
    "    worddicts = {}\n",
    "    worddicts['Monica'] = {}\n",
    "    worddicts[\"Rachel\"]={}\n",
    "    worddicts[\"Phoebe\"]={}\n",
    "    worddicts[\"Chandler\"]={} \n",
    "    worddicts[\"Ross\"]={} \n",
    "    worddicts[\"Joey\"]={}\n",
    "    for i, j in enumerate(df[\"Person\"]):\n",
    "        if j in worddicts:\n",
    "            wordlist = Counter((x.rstrip(punctuation).lower() for x in df.iloc[i][1].split()))\n",
    "            for word in wordlist:\n",
    "                try:\n",
    "                    worddicts[j][word] = worddicts[j][word]+1\n",
    "                except KeyError:\n",
    "                    worddicts[j][word] = 1\n",
    "                    \n",
    "    for p in worddicts:\n",
    "        for key in worddicts[p].keys():\n",
    "            try:\n",
    "                mostcommon[p][key] = mostcommon[p][key]+worddicts[p][key]\n",
    "            except:\n",
    "                mostcommon[p][key] = worddicts[p][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This window is for creating a list of the most common words used by all characters throughout the series\n",
    "mostcommonall = {}\n",
    "for d in mostcommon:\n",
    "    mostcommonall = {k: mostcommonall.get(k, 0) + mostcommon[d].get(k, 0) \n",
    "                     for k in set(mostcommonall) | set(mostcommon[d])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This window creates a dataframe with every feature\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from heapq import nlargest\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "master_array = [] #append results to array to create data frame\n",
    "\n",
    "for filename in os.listdir('season/'):\n",
    "    if filename.endswith(\".html\"):\n",
    "        split_name = filename.split('.') #obtain the season and episodes\n",
    "        season = split_name[0]   \n",
    "        episode = split_name[1]\n",
    "        #TODO method to get each line of the script\n",
    "        master_array.append([season, episode])\n",
    "    else:\n",
    "        pass\n",
    "numblines = dict()\n",
    "numblines[\"Episode\"] = []\n",
    "numblines[\"Number of lines\"] = []\n",
    "numblines[\"Number of words\"] = []\n",
    "numblines[\"Different words used\"] = []\n",
    "numblines[\"Words per line\"] = []\n",
    "numblines[\"Different words ratio\"] = []\n",
    "numblines[\"Exclamation marks\"] = []\n",
    "numblines[\"Question marks\"] = []\n",
    "numblines[\"Periods\"] = []\n",
    "numblines[\"Apostrophe\"] = []\n",
    "numblines[\"Pronouns\"] = []\n",
    "numblines[\"Pronouns per line\"] = []\n",
    "numblines[\"Commas\"] = []\n",
    "numblines[\"Commas per line\"] = []\n",
    "numblines[\"Word\"] = []\n",
    "numblines[\"Word2\"] = []\n",
    "numblines[\"Phrase\"] = []\n",
    "numblines[\"Phrase2\"] = []\n",
    "numblines[\"Phrase3\"] = []\n",
    "numblines[\"Phrase4\"] = []\n",
    "numblines[\"Most common Monica\"] = []\n",
    "numblines[\"Most common Rachel\"] = []\n",
    "numblines[\"Most common Phoebe\"] = []\n",
    "numblines[\"Most common Chandler\"] = []\n",
    "numblines[\"Most common Ross\"] = []\n",
    "numblines[\"Most common Joey\"] = []\n",
    "numblines[\"Name Monica\"] = []\n",
    "numblines[\"Name Rachel\"] = []\n",
    "numblines[\"Name Phoebe\"] = []\n",
    "numblines[\"Name Chandler\"] = []\n",
    "numblines[\"Name Ross\"] = []\n",
    "numblines[\"Name Joey\"] = []\n",
    "numblines[\"Name total\"] = []\n",
    "#numblines[\"Most common 30\"] = []\n",
    "#numblines[\"Most common 50\"] = []\n",
    "numblines[\"Character\"] = []\n",
    "\n",
    "loop = 0\n",
    "for num, ep in enumerate(master_array):\n",
    "#for num in [0]:\n",
    "    filename=master_array[num][0]+'.'+master_array[num][1]\n",
    "\n",
    "    f = open('season/'+filename, 'r')\n",
    "    data = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(data)\n",
    "    readsoup = BeautifulSoup(data)\n",
    "\n",
    "    soup = soup.get_text().translate(str.maketrans({'\\n': ' ', '\\xa0': ''}))\n",
    "    pattern = re.compile(r'\\s(?=\\w+(?=:))') # store the regex\n",
    "    result = re.split(pattern, soup) # split the script where our pattern matched (pink dot)\n",
    "\n",
    "\n",
    "    episode_array=[]\n",
    "    for item in result:\n",
    "        split_line = item.split(': ')\n",
    "        try:\n",
    "            character = split_line[0]\n",
    "            speech = split_line[1]\n",
    "            episode_array.append([character, speech])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    df = pd.DataFrame(episode_array, columns = ['Person','Said'])\n",
    "\n",
    "    df['Person'].replace({'CHAN':'Chandler','CHANDLER':'Chandler', 'Chandlers':'Chandler',\n",
    "                        'JOEY':'Joey',\n",
    "                        'MNCA':'Monica','MONICA':'Monica',\n",
    "                        'PHOE':'Phoebe','PHOEBE':'Phoebe', 'Pheebs':'Phoebe',\n",
    "                        'Rache':'Rachel','RACHEL':'Rachel', 'RACH':'Rachel',\n",
    "                        'ROSS':'Ross'},inplace=True)\n",
    "\n",
    "    curved = re.compile(\"(\\(.*?\\))\")\n",
    "    brackets = re.compile(\"(\\[.*?\\])\")\n",
    "    scene = re.compile(\"\\[Scene\")\n",
    "    cbreak = re.compile(\"Commercial break\", re.IGNORECASE)\n",
    "    theend = re.compile(\"The end\", re.IGNORECASE)\n",
    "    for j,i in enumerate(df['Said']):\n",
    "        df.iloc[j][1] =re.sub(curved, '', i)\n",
    "\n",
    "    for j,i in enumerate(df['Said']):\n",
    "        df.iloc[j][1] =re.sub(brackets, '', i)\n",
    "        \n",
    "    for j,i in enumerate(df['Said']):\n",
    "        df.iloc[j][1] =re.sub(scene, '', i)\n",
    "        \n",
    "    for j,i in enumerate(df['Said']):\n",
    "        df.iloc[j][1] =re.sub(cbreak, '', i)\n",
    "        \n",
    "    for j,i in enumerate(df['Said']):\n",
    "        df.iloc[j][1] =re.sub(theend, '', i)\n",
    "\n",
    "    worddicts = {}\n",
    "    worddicts['Monica'] = {}\n",
    "    worddicts[\"Rachel\"]={}\n",
    "    worddicts[\"Phoebe\"]={}\n",
    "    worddicts[\"Chandler\"]={} \n",
    "    worddicts[\"Ross\"]={} \n",
    "    worddicts[\"Joey\"]={}\n",
    "    for i, j in enumerate(df[\"Person\"]):\n",
    "        if j in worddicts:\n",
    "            wordlist = Counter((x.rstrip(punctuation).lower() for x in df.iloc[i][1].split()))\n",
    "            for word in wordlist:\n",
    "                try:\n",
    "                    worddicts[j][word] = worddicts[j][word]+1\n",
    "                except KeyError:\n",
    "                    worddicts[j][word] = 1\n",
    "    \n",
    "    namedict=dict()\n",
    "    namedict[\"Rachel\"]=[0]*11\n",
    "    namedict[\"Phoebe\"]=[0]*11\n",
    "    namedict[\"Chandler\"]=[0]*11\n",
    "    namedict[\"Ross\"]=[0]*11\n",
    "    namedict[\"Joey\"]=[0]*11\n",
    "    namedict[\"Monica\"]=[0]*11\n",
    "    \n",
    "    commonwords = {}\n",
    "    commonwords['Monica']={}\n",
    "    commonwords[\"Rachel\"]={}\n",
    "    commonwords[\"Phoebe\"]={}\n",
    "    commonwords[\"Chandler\"]={} \n",
    "    commonwords[\"Ross\"]={} \n",
    "    commonwords[\"Joey\"]={}\n",
    "    for b in namedict:\n",
    "        for c in namedict:\n",
    "            commonwords[b][c] = 0\n",
    "\n",
    "    k = 0\n",
    "    for i in df[\"Person\"]:\n",
    "        if i in namedict:\n",
    "            try:\n",
    "                namedict[i][0] = namedict[i][0]+1\n",
    "                namedict[i][1] = namedict[i][1]+Counter(df.iloc[k][1])['!']\n",
    "                namedict[i][2] = namedict[i][2]+Counter(df.iloc[k][1])['?']\n",
    "                if \"oh my god\" in df.iloc[k][1].lower():\n",
    "                    namedict[i][3] = namedict[i][3]+1\n",
    "                namedict[i][4] = namedict[i][4]+Counter(df.iloc[k][1])[',']\n",
    "                namedict[i][5] = namedict[i][5]+Counter(df.iloc[k][1])[\"'\"]\n",
    "                namedict[i][6] = namedict[i][6]+Counter(df.iloc[k][1])[\".\"]\n",
    "                text = word_tokenize(df.iloc[k][1])\n",
    "                tags = pos_tag(text)\n",
    "                for tag in range(len(tags)):\n",
    "                    if tags[tag][1] == 'PRP':\n",
    "                        namedict[i][7]+=1\n",
    "                if \"how you doin\" in df.iloc[k][1].lower():\n",
    "                    namedict[i][8] = namedict[i][8]+1\n",
    "                if \"could you be more\" in df.iloc[k][1].lower():\n",
    "                    namedict[i][9] = namedict[i][9]+1\n",
    "                if \"we were on a break\" in df.iloc[k][1].lower():\n",
    "                    namedict[i][10] = namedict[i][10]+1\n",
    "            except:\n",
    "                pass\n",
    "        k = k+1\n",
    "            \n",
    "    \n",
    "    for p in namedict:\n",
    "        for person in namedict:\n",
    "            try:\n",
    "                numblines[\"Name \"+person].append(worddicts[p][person.lower()])\n",
    "            except:\n",
    "                numblines[\"Name \"+person].append(0)\n",
    "            for word in nlargest(30, worddicts[person], key=worddicts[person].get):\n",
    "                if word in nlargest(30, mostcommon[p], key=mostcommon[p].get):\n",
    "                    try:\n",
    "                        commonwords[p][person] = commonwords[p][person]+worddicts[person][word]\n",
    "                    except:\n",
    "                        commonwords[p][person] = worddicts[p][word]\n",
    "                else:\n",
    "                    pass\n",
    "        try:\n",
    "            numblines[\"Number of lines\"].append(namedict[p][0])\n",
    "            numblines[\"Character\"].append(p)\n",
    "            numblines[\"Episode\"].append(master_array[num][0])\n",
    "            #numblines[\"Most common 10\"].append(nlargest(10, worddicts[p], key=worddicts[p].get))\n",
    "            #numblines[\"Most common 30\"].append(nlargest(30, worddicts[p], key=worddicts[p].get))\n",
    "            #numblines[\"Most common 50\"].append(nlargest(50, worddicts[p], key=worddicts[p].get))\n",
    "            numblines[\"Number of words\"].append(sum(worddicts[p].values()))\n",
    "            numblines[\"Different words used\"].append(len(worddicts[p]))\n",
    "            numblines[\"Exclamation marks\"].append(namedict[p][1])\n",
    "            numblines[\"Question marks\"].append(namedict[p][2])\n",
    "            numblines[\"Words per line\"].append(sum(worddicts[p].values())/namedict[p][0])\n",
    "            numblines[\"Different words ratio\"].append(len(worddicts[p])/sum(worddicts[p].values()))\n",
    "            numblines[\"Most common Monica\"].append(commonwords[p][\"Monica\"]/sum(worddicts[p].values()))\n",
    "            numblines[\"Most common Rachel\"].append(commonwords[p][\"Rachel\"]/sum(worddicts[p].values()))\n",
    "            numblines[\"Most common Phoebe\"].append(commonwords[p][\"Phoebe\"]/sum(worddicts[p].values()))\n",
    "            numblines[\"Most common Chandler\"].append(commonwords[p][\"Chandler\"]/sum(worddicts[p].values()))\n",
    "            numblines[\"Most common Ross\"].append(commonwords[p][\"Ross\"]/sum(worddicts[p].values()))\n",
    "            numblines[\"Most common Joey\"].append(commonwords[p][\"Joey\"]/sum(worddicts[p].values()))\n",
    "            numblines[\"Name total\"].append(numblines[\"Name Monica\"][loop]+\n",
    "                                              numblines[\"Name Joey\"][loop]+\n",
    "                                              numblines[\"Name Rachel\"][loop]+\n",
    "                                              numblines[\"Name Chandler\"][loop]+\n",
    "                                              numblines[\"Name Phoebe\"][loop]+\n",
    "                                              numblines[\"Name Ross\"][loop])\n",
    "            numblines[\"Phrase\"].append(namedict[p][3])\n",
    "            numblines[\"Commas\"].append(namedict[p][4])\n",
    "            numblines[\"Commas per line\"].append(namedict[p][4]/namedict[p][0])\n",
    "            numblines[\"Apostrophe\"].append(namedict[p][5])\n",
    "            numblines[\"Periods\"].append(namedict[p][6])\n",
    "            numblines[\"Pronouns\"].append(namedict[p][7])\n",
    "            numblines[\"Pronouns per line\"].append(namedict[p][7]/namedict[p][0])\n",
    "            numblines[\"Phrase2\"].append(namedict[p][8])\n",
    "            numblines[\"Phrase3\"].append(namedict[p][9])\n",
    "            numblines[\"Phrase4\"].append(namedict[p][10])\n",
    "            \n",
    "            try:\n",
    "                numblines[\"Word\"].append(worddicts[p][\"dude\"])\n",
    "            except:\n",
    "                numblines[\"Word\"].append(0)\n",
    "            try:\n",
    "                numblines[\"Word2\"].append(worddicts[p][\"crazy\"])\n",
    "            except:\n",
    "                numblines[\"Word2\"].append(0)\n",
    "            \n",
    "        except:\n",
    "               pass\n",
    "        loop = loop+1\n",
    "        \n",
    "epinfo = pd.DataFrame.from_dict(numblines, orient='index')\n",
    "epinfo = epinfo.transpose()\n",
    "epinfo.to_csv('epinfo.csv', index=False)\n",
    "epinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96154b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This window creates a different dataframe with mostly features corresponding to single words\n",
    "###Import all modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup as htmltotxt\n",
    "\n",
    "import sklearn.preprocessing as skl_pre\n",
    "import sklearn.linear_model as skl_lm\n",
    "import sklearn.discriminant_analysis as skl_da\n",
    "import sklearn.neighbors as skl_nb\n",
    "\n",
    "###Creates dataframe with features\n",
    "master_dict = dict()\n",
    "feature_dict = dict()\n",
    "num_words = 100\n",
    "\n",
    "single_words = nlargest(num_words, mostcommonall, key=mostcommonall.get)\n",
    "Features = [\"Number of lines\",\"Number of words\",\"Words per line\",\"Exclamation marks\",\"Question marks\",\"Character\",\n",
    "           \"unique_words\"]\n",
    "Features.extend(single_words)\n",
    "for feat in Features:\n",
    "    master_dict[feat] = []\n",
    "    feature_dict[feat] = 0\n",
    "\n",
    "chars = ['rachel','phoebe','chandler','ross','joey','monica']\n",
    "\n",
    "\n",
    "for filename in os.listdir('season/'):  \n",
    "    f = open('season/'+filename, 'r')\n",
    "\n",
    "    data = htmltotxt(f.read()).get_text() #reads f as html file and converts into txt\n",
    "    \n",
    "    name_dict = dict()\n",
    "    for char in chars:\n",
    "        name_dict[char] = feature_dict.copy()\n",
    "\n",
    "    pattern = re.compile(r'\\s(?=\\w+(?=:))') #store the regex\n",
    "    result = re.split(pattern, data) # split the script where our pattern matched (pink dot)\n",
    "\n",
    "    episode_array=[]\n",
    "    for item in result:\n",
    "        split_line = item.split(':')\n",
    "        try:\n",
    "            character = split_line[0]\n",
    "            speech = split_line[1]\n",
    "            episode_array.append([character, speech])\n",
    "        except:\n",
    "            pass\n",
    "    unique_words_char = {'monica': []\n",
    "              ,'joey': []\n",
    "              ,'phoebe': []\n",
    "              ,'rachel': []\n",
    "              ,'ross': []\n",
    "              ,'chandler':[]}\n",
    "    \n",
    "    i = 0\n",
    "    while(len(episode_array) > i):\n",
    "        if episode_array[i][0].lower() not in chars:\n",
    "            del episode_array[i]\n",
    "        else:\n",
    "            character =episode_array[i][0].lower()\n",
    "            sentence =episode_array[i][1].lower()     \n",
    "            sentence = sentence.replace(\"-\",\"\").replace(\"\\n\",\" \").replace(\"\\xa0\",\" \").replace(\",\",\"\").replace(\".\",\"\").replace(\":\",\"\").replace(\"'\",\"\").replace(\"opening credits\",\"\").replace(\"closing credits\",\"\").replace(\"ending credits\",\"\").replace(\"commercial break\",\"\").replace(\"\\x92\",\"\")\n",
    "            sentence = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", sentence) #removes everything withing () and []\n",
    "            sentence = re.sub('\\[.*', '', sentence)\n",
    "            sentence = re.sub('\\(.*', '', sentence)\n",
    "            sentence = sentence.strip()\n",
    "            if len(episode_array) == i+1: #last sentence\n",
    "                sentence = sentence[:len(sentence)-3] #delets \"end\" in the end\n",
    "                sentence = sentence.strip()\n",
    "\n",
    "            words = sentence.split()\n",
    "            \n",
    "            \n",
    "            \n",
    "            name_dict[character][\"Number of lines\"] +=1\n",
    "            name_dict[character][\"Number of words\"] +=len(words)\n",
    "            name_dict[character][\"Exclamation marks\"] += sentence.count('!')\n",
    "            name_dict[character][\"Question marks\"] += sentence.count('?')\n",
    "            \n",
    "            episode_array[i][1].replace('chan','chandler').replace('chandlers','chandler').replace('mnca','monica').replace('phoe','phoebe').replace('pheebs','phoebe').replace('rache','rachel').replace('rach','rachel')\n",
    "            for word in words:\n",
    "                if word not in unique_words_char[character]:\n",
    "                    unique_words_char[character].append(word)\n",
    "            \n",
    "            for single_word in single_words:\n",
    "                name_dict[character][single_word] += sentence.count(single_word)\n",
    "            \n",
    "                \n",
    "            i +=1\n",
    "    for char in chars:\n",
    "        if name_dict[char][\"Number of lines\"] != 0:\n",
    "            name_dict[char][\"Words per line\"] = name_dict[char][\"Number of words\"]/name_dict[char][\"Number of lines\"]\n",
    "\n",
    "        master_dict[\"Number of lines\"].append(name_dict[char][\"Number of lines\"])\n",
    "        master_dict[\"Number of words\"].append(name_dict[char][\"Number of words\"])\n",
    "        master_dict[\"Words per line\"].append(name_dict[char][\"Words per line\"])\n",
    "        master_dict[\"Exclamation marks\"].append(name_dict[char][\"Exclamation marks\"])\n",
    "        master_dict[\"Question marks\"].append(name_dict[char][\"Question marks\"])\n",
    "        master_dict[\"Character\"].append(char)\n",
    "        master_dict[\"unique_words\"].append(len(unique_words_char[char]))\n",
    "        for single_word in single_words:\n",
    "            master_dict[single_word].append(name_dict[char][single_word])\n",
    "\n",
    "epinfo2 = pd.DataFrame.from_dict(master_dict, orient='index')\n",
    "\n",
    "pd.set_option('display.max_rows', epinfo2.transpose().shape[0]+1)\n",
    "epinfo2 = epinfo2.transpose()\n",
    "epinfo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This window creates a machine learning model and produces some results\n",
    "import xgboost as xgb\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "import sklearn.linear_model as skl_lm\n",
    "import sklearn.discriminant_analysis as skl_da\n",
    "import sklearn.neighbors as skl_nnb\n",
    "import sklearn.tree as skl_tr\n",
    "import sklearn.neural_network as skl_nn\n",
    "import sklearn.ensemble as skl_em\n",
    "import sklearn.naive_bayes as skl_nb\n",
    "import sklearn.svm as skl_svm\n",
    "import sklearn.model_selection as skl_ms\n",
    "import sklearn.inspection as skl_in\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as skl_pre\n",
    "import math\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "epinfo = pd.read_csv('epinfo.csv')\n",
    "num_words = 100\n",
    "for word in nlargest(num_words, mostcommonall, key=mostcommonall.get):\n",
    "    epinfo = epinfo.join(epinfo2[word])\n",
    "    epinfo[word] = pd.to_numeric(epinfo[word])\n",
    "features = [#'Episode', \n",
    "            'Number of lines', \n",
    "            'Number of words', \n",
    "            'Different words used',\n",
    "            'Words per line', \n",
    "            'Different words ratio',\n",
    "            'Exclamation marks', \n",
    "            'Question marks',\n",
    "            'Periods',\n",
    "            'Apostrophe',\n",
    "            'Pronouns',\n",
    "            'Pronouns per line',\n",
    "            'Commas',\n",
    "            'Commas per line',\n",
    "            'Word',\n",
    "            'Word2',\n",
    "            'Phrase',\n",
    "            #'Phrase2',\n",
    "            #'Phrase3',\n",
    "            #'Phrase4',\n",
    "            \"Most common Monica\",\n",
    "            \"Most common Rachel\",\n",
    "            \"Most common Phoebe\",\n",
    "            \"Most common Chandler\",\n",
    "            \"Most common Ross\",\n",
    "            \"Most common Joey\",\n",
    "            \"Name Monica\",\n",
    "            \"Name Rachel\",\n",
    "            \"Name Phoebe\",\n",
    "            \"Name Joey\",\n",
    "            \"Name Chandler\",\n",
    "            \"Name Ross\",\n",
    "            \"Name total\",\n",
    "             ]\n",
    "#features = [] #Use only to test most common single word features\n",
    "features.extend(nlargest(num_words, mostcommonall, key=mostcommonall.get))\n",
    "num_features = len(features)\n",
    "n_fold = 10\n",
    "n_comp = len(features)\n",
    "numneigh=7\n",
    "Y_encode = [4, 3, 0, 5, 1, 2]*229\n",
    "def aic(model, Y, Y_hat):\n",
    "    k = model.coef_.size + model.get_params()['fit_intercept'] + 1\n",
    "    n = Y.size\n",
    "    loglik = np.sum(np.log(model.predict_proba(X)[np.arange(n), Y_encode]))\n",
    "    return 2 * (k - loglik)\n",
    "\n",
    "#Every model that was tried\n",
    "#model = skl_lm.LogisticRegression(solver='newton-cg', multi_class='multinomial', max_iter=10000, \n",
    "#                                                                penalty='l2', fit_intercept = False)\n",
    "#model = skl_da.QuadraticDiscriminantAnalysis()\n",
    "#model = skl_nnb.KNeighborsClassifier(n_neighbors=numneigh)\n",
    "#model = skl_tr.DecisionTreeClassifier()\n",
    "#model = skl_nn.MLPClassifier(solver = 'adam', max_iter=200, )\n",
    "#model = skl_em.AdaBoostClassifier(n_estimators=50)\n",
    "#model = skl_em.BaggingClassifier(n_estimators=100)\n",
    "model = skl_em.GradientBoostingClassifier(warm_start=True)\n",
    "#model = skl_em.ExtraTreesClassifier()\n",
    "#model = skl_em.HistGradientBoostingClassifier()\n",
    "#model = skl_nb.ComplementNB()\n",
    "#model = skl_svm.LinearSVC(max_iter = 10000)\n",
    "#model = skl_em.RandomForestClassifier()\n",
    "#model = xgb.XGBClassifier(eval_metric='mlogloss')\n",
    "\n",
    "primeacc = [0]\n",
    "primeAIC = [np.inf]\n",
    "best_features = []\n",
    "best_fAIC = []\n",
    "#for features in itertools.combinations(factors, num_features):\n",
    "#for numneigh in range(1, 50):\n",
    "\n",
    "#Training model\n",
    "X = epinfo[list(features)]\n",
    "Y = epinfo['Character']\n",
    "cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)\n",
    "accuracy = []\n",
    "for train_index, val_index in cv.split(X):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    Y_train, Y_val = Y.iloc[train_index], Y.iloc[val_index]\n",
    "    \n",
    "    ###Used for logistic regression and PCA only\n",
    "    #scaler = skl_pre.StandardScaler()\n",
    "    #X_train = scaler.fit_transform(X_train)\n",
    "    #X_val = scaler.transform(X_val)\n",
    "    #pca = PCA(n_components = n_comp)\n",
    "    #X_train = pca.fit_transform(X_train)\n",
    "    #X_val = pca.transform(X_val)\n",
    "    \n",
    "    model.fit(X_train, Y_train)\n",
    "    prediction = model.predict(X_val)\n",
    "    accuracy.append(np.mean(prediction==Y_val))\n",
    "if np.mean(accuracy) > max(primeacc):\n",
    "    best_features = list(features)\n",
    "    #explained_variance = pca.explained_variance_ratio_\n",
    "    principalComponents = X_train\n",
    "\n",
    "###AIC calculation\n",
    "#if aic(model, Y, Y_encode) < min(primeAIC):\n",
    "    #best_fAIC = list(features)\n",
    "#primeAIC.append(aic(model, Y, Y_encode))\n",
    "\n",
    "primeacc.append(np.mean(accuracy))\n",
    "\n",
    "#Printing different results\n",
    "print(np.max(primeacc))\n",
    "#print(model.feature_importances_)\n",
    "#print(skl_in.permutation_importance(model, X_val, Y_val))\n",
    "#print(np.min(primeAIC))\n",
    "#print('Best features for accuracy')\n",
    "#print(best_features)\n",
    "#print(pd.crosstab(prediction, Y_val))\n",
    "#print('Best features for low AIC')\n",
    "#print(best_fAIC)\n",
    "\n",
    "#PCA plotting\n",
    "pcplot = []\n",
    "for i in range(n_comp):\n",
    "    pcplot.append(f'PC{i+1}')\n",
    "#plt.bar(pcplot, explained_variance)\n",
    "#plt.show()\n",
    "\n",
    "#plt.figure(figsize=(15, 15), dpi=100)\n",
    "#print(pca.explained_variance_ratio_)\n",
    "#principalDf = pd.DataFrame(data = principalComponents\n",
    "#             , columns = pcplot)#['principal component 1', 'principal component 2','principal component 3'])\n",
    "#plt.matshow(pca.components_,cmap='viridis', fignum=1)\n",
    "#plt.yticks(list(range(n_comp)), pcplot#['1st Comp','2nd Comp','3rd Comp']\n",
    "#           ,fontsize=7)\n",
    "#plt.xticks(list(range(n_comp)), features#['1st Comp','2nd Comp','3rd Comp']\n",
    "#           ,fontsize=7, rotation=90)\n",
    "#plt.colorbar()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea07931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This window plots two features against each other\n",
    "from matplotlib.lines import Line2D\n",
    "feature1 = \"Most common Ross\"\n",
    "feature2 = \"Most common Rachel\"\n",
    "xax = X_val[feature1].to_list()\n",
    "yax = X_val[feature2].to_list()\n",
    "#plt.figure(figsize=(8,6), dpi=100)\n",
    "#plt.scatter(xax, yax)\n",
    "\n",
    "colors=[]\n",
    "for entry in prediction:\n",
    "    if entry == \"Rachel\":\n",
    "        colors.append(\"orange\")#orange\n",
    "    elif entry == \"Monica\":\n",
    "        colors.append(\"lightskyblue\")#lightskyblue #white\n",
    "    elif entry == \"Phoebe\":\n",
    "        colors.append(\"yellow\")#yellow\n",
    "    elif entry == \"Ross\":\n",
    "        colors.append(\"grey\")#grey\n",
    "    elif entry == \"Chandler\":\n",
    "        colors.append(\"black\")#black\n",
    "    else:\n",
    "        colors.append(\"blue\")#blue\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Rachel', markerfacecolor='orange', markersize=10),\n",
    "                  Line2D([0], [0], marker='o', color='w', label='Monica', markerfacecolor='lightskyblue', markersize=10),\n",
    "                  Line2D([0], [0], marker='o', color='w', label='Phoebe', markerfacecolor='yellow', markersize=10),\n",
    "                  Line2D([0], [0], marker='o', color='w', label='Ross', markerfacecolor='grey', markersize=10),\n",
    "                  Line2D([0], [0], marker='o', color='w', label='Chandler', markerfacecolor='black', markersize=10),\n",
    "                  Line2D([0], [0], marker='o', color='w', label='Joey', markerfacecolor='blue', markersize=10),]\n",
    "fig, ax = plt.subplots(figsize=(8,6), dpi=100)\n",
    "ax.scatter(xax, yax, c = colors)\n",
    "ax.set_xlabel(feature1)\n",
    "ax.set_ylabel(feature2)\n",
    "ax.legend(handles = legend_elements),\n",
    "\n",
    "#Text label for every data point\n",
    "#for i, txt in enumerate(prediction):\n",
    "#    ax.annotate(txt, (xax[i], yax[i]))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92162824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This window is for extracting data features from a different website in order to make a prediction using the\n",
    "#Gradient Boosting model\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "numblines = dict()\n",
    "numblines[\"Number of words\"] = 0\n",
    "numblines[\"Different words used\"] = 0\n",
    "numblines[\"Exclamation marks\"] = 0\n",
    "numblines[\"Question marks\"] = 0\n",
    "numblines[\"Most common Monica\"] = 0\n",
    "numblines[\"Most common Rachel\"] = 0\n",
    "numblines[\"Most common Phoebe\"] = 0\n",
    "numblines[\"Most common Chandler\"] = 0\n",
    "numblines[\"Most common Ross\"] = 0\n",
    "numblines[\"Most common Joey\"] = 0\n",
    "\n",
    "URL = 'https://soccermatics.medium.com/how-analytics-makes-football-more-fun-5eb5138e6600'\n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "results = soup.find(id=\"0f5a\")\n",
    "texten = results.get_text()\n",
    "result2 = soup.find(id=\"ee03\")\n",
    "texten2 = result2.get_text()\n",
    "\n",
    "\n",
    "alltext= texten+texten2\n",
    "\n",
    "wordlistan = Counter((x.rstrip(punctuation).lower() for x in alltext.split()))\n",
    "wordlistan = dict(wordlistan)\n",
    "\n",
    "newtextfeature = [0]*10\n",
    "newtextfeature[0] = sum(wordlistan.values()) #\"Number of words\"\n",
    "newtextfeature[1] = len(wordlistan) #Different words used\n",
    "newtextfeature[2] = Counter(alltext)[\"!\"] #\"Exclamation marks\"\n",
    "newtextfeature[3] = Counter(alltext)[\"?\"] #\"Question marks\"\n",
    "\n",
    "ntfcw = dict()\n",
    "ntfcw[\"Monica\"] = 0\n",
    "ntfcw[\"Rachel\"] = 0\n",
    "ntfcw[\"Phoebe\"] = 0\n",
    "ntfcw[\"Chandler\"] = 0\n",
    "ntfcw[\"Ross\"] = 0\n",
    "ntfcw[\"Joey\"] = 0\n",
    "\n",
    "for person in worddicts:\n",
    "    for word in nlargest(30, worddicts[person], key=worddicts[person].get):\n",
    "        if word in nlargest(30, wordlistan, key=wordlistan.get):\n",
    "            try:\n",
    "                ntfcw[person] = ntfcw[person]+worddicts[person][word]\n",
    "            except:\n",
    "                ntfcw[person] = worddicts[person][word]\n",
    "\n",
    "newtextfeature[4] = ntfcw[\"Monica\"] #\"Most common Monica\"\n",
    "newtextfeature[5] = ntfcw[\"Rachel\"] #\"Most common Rachel\"\n",
    "newtextfeature[6] = ntfcw[\"Phoebe\"] #\"Most common Phoebe\"\n",
    "newtextfeature[7] = ntfcw[\"Chandler\"] #\"Most common Chandler\"\n",
    "newtextfeature[8] = ntfcw[\"Ross\"] #\"Most common Ross\"\n",
    "newtextfeature[9] = ntfcw[\"Joey\"] #\"Most common Joey\"\n",
    "\n",
    "numblines[\"Number of words\"] = newtextfeature[0]\n",
    "numblines[\"Different words used\"] = newtextfeature[1]\n",
    "numblines[\"Exclamation marks\"] = newtextfeature[2]\n",
    "numblines[\"Question marks\"] = newtextfeature[3]\n",
    "numblines[\"Most common Monica\"] = newtextfeature[4]\n",
    "numblines[\"Most common Rachel\"] = newtextfeature[5]\n",
    "numblines[\"Most common Phoebe\"] = newtextfeature[6]\n",
    "numblines[\"Most common Chandler\"] = newtextfeature[7]\n",
    "numblines[\"Most common Ross\"] = newtextfeature[8]\n",
    "numblines[\"Most common Joey\"] = newtextfeature[9]\n",
    "\n",
    "epinfo3 = pd.DataFrame.from_dict(numblines, orient='index').transpose()\n",
    "epinfo3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a528dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This window uses the model coefficients and tells you which features are the most important for which character\n",
    "characters = ['chandler','joey','monica','phoebe','rachel','ross']\n",
    "Chandler =0\n",
    "Joey = 0\n",
    "Monica = 0\n",
    "Phoebe = 0\n",
    "Rachel = 0\n",
    "Ross = 0    \n",
    "    \n",
    "for character in range(len(model.coef_)):\n",
    "    print(f'{characters[character]}´s most characteristic feature:')\n",
    "    print(list(X.columns)[np.argmax(model.coef_[character])])\n",
    "print(' ')\n",
    "for character in range(len(model.coef_)):\n",
    "    print(f'{characters[character]}´s least characteristic feature:')\n",
    "    print(list(X.columns)[np.argmin(model.coef_[character])])    \n",
    "\n",
    "print(' ')\n",
    "for character in range(len(model.coef_)):\n",
    "    list1 = list(model.coef_[character])\n",
    "    list2 = list(X.columns)\n",
    "    list1, list2 = (list(t) for t in zip(*sorted(zip(list1, list2),reverse=True)))\n",
    "    print(f'{characters[character]}´s most characteristic words in ascending order:')\n",
    "    print(list2)\n",
    "    print(list1)\n",
    "    print(' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
